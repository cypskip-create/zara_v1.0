## â€œâ€â€
data_pipeline.py AfriCode LM Data Pipeline

Collects, cleans, and prepares training data specifically for the
African Code Assistant, including:

- African API documentation (M-Pesa, Paystack, Flutterwave, MTN MoMo, Airtel Money)
- Code from GitHub repositories
- HuggingFace coding datasets
- African tech blog content
- Custom Q&A pairs for fine-tuning

Usage:
# African API docs + web scraping
python data_pipeline.py â€“source web â€“urls africode_urls.txt â€“output africode

```
# HuggingFace coding dataset (recommended first step)
python data_pipeline.py --source hf --dataset bigcode/the-stack-smol --output africode

# Your own collected files
python data_pipeline.py --source files --input_dir ./raw_data --output africode

# Fine-tuning Q&A pairs
python data_pipeline.py --source qa --input_dir ./qa_pairs --output africode_ft
```

Requirements:
pip install tiktoken datasets requests beautifulsoup4 tqdm numpy
â€œâ€â€

import os
import re
import sys
import json
import hashlib
import argparse
import numpy as np
from pathlib import Path
from tqdm import tqdm
from typing import List, Iterator

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

# African API & Tech URLs â€” Pre-loaded for AfriCode Training

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

AFRICODE_URLS = [
# M-Pesa / Safaricom
â€œhttps://developer.safaricom.co.ke/APIsâ€,
â€œhttps://developer.safaricom.co.ke/Documentationâ€,

```
# Paystack
"https://paystack.com/docs/api/",
"https://paystack.com/docs/payments/accept-payments/",
"https://paystack.com/docs/libraries-and-plugins/",

# Flutterwave
"https://developer.flutterwave.com/docs",
"https://developer.flutterwave.com/reference",

# MTN MoMo
"https://momodeveloper.mtn.com/docs",

# African tech blogs
"https://engineering.paystack.com",
"https://medium.com/andela",
```

]

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

# Argument Parsing

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

def parse_args():
parser = argparse.ArgumentParser(description=â€œAfriCode LM Data Pipelineâ€)
parser.add_argument(â€â€“sourceâ€,     type=str, required=True,
choices=[â€œfilesâ€, â€œwebâ€, â€œhfâ€, â€œqaâ€, â€œafricodeâ€],
help=â€œData source type:\nâ€
â€œ  files    â€” local .txt/.json files\nâ€
â€œ  web      â€” scrape URLs from a file\nâ€
â€œ  hf       â€” HuggingFace dataset\nâ€
â€œ  qa       â€” Q&A pairs for fine-tuning\nâ€
â€œ  africode â€” auto-scrape African API docsâ€)
parser.add_argument(â€â€“input_dirâ€,      type=str, default=â€./raw_dataâ€)
parser.add_argument(â€â€“urlsâ€,           type=str, default=â€œafricode_urls.txtâ€)
parser.add_argument(â€â€“datasetâ€,        type=str, default=â€œbigcode/the-stack-smolâ€)
parser.add_argument(â€â€“dataset_configâ€, type=str, default=â€œpythonâ€,
help=â€œLanguage subset for code datasets (python, javascript, etc.)â€)
parser.add_argument(â€â€“outputâ€,         type=str, default=â€œafricodeâ€)
parser.add_argument(â€â€“output_dirâ€,     type=str, default=â€./processedâ€)
parser.add_argument(â€â€“min_lengthâ€,     type=int, default=50)
parser.add_argument(â€â€“max_lengthâ€,     type=int, default=50_000)
parser.add_argument(â€â€“val_fracâ€,       type=float, default=0.05)
parser.add_argument(â€â€“analyzeâ€,        action=â€œstore_trueâ€)
return parser.parse_args()

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

# Text Cleaning â€” Code-Aware

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

class TextCleaner:
â€œâ€â€œCleans text while preserving code structure (indentation, syntax).â€â€â€

```
def __init__(self, min_length=50, max_length=50_000):
    self.min_length = min_length
    self.max_length = max_length

def clean(self, text: str) -> str:
    if not text or not isinstance(text, str):
        return ""

    # Normalize line endings
    text = text.replace("\r\n", "\n").replace("\r", "\n")

    # Remove null bytes and control characters (preserve tabs for code indentation)
    text = re.sub(r"[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]", "", text)

    # Collapse excessive blank lines (keep up to 2 for code readability)
    text = re.sub(r"\n{4,}", "\n\n\n", text)

    return text.strip()

def is_valid(self, text: str) -> bool:
    if len(text) < self.min_length:
        return False
    # For code: allow lower alpha ratio (code has symbols, numbers, brackets)
    alpha_ratio = sum(c.isalpha() for c in text) / max(len(text), 1)
    if alpha_ratio < 0.1:  # very low threshold for code files
        return False
    return True

def truncate(self, text: str) -> str:
    if len(text) > self.max_length:
        return text[:self.max_length]
    return text

def process(self, text: str):
    text = self.clean(text)
    if not self.is_valid(text):
        return None
    return self.truncate(text)
```

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

# Deduplication

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

class Deduplicator:
def **init**(self):
self.seen = set()

```
def is_duplicate(self, text: str) -> bool:
    h = hashlib.md5(text.encode("utf-8")).hexdigest()
    if h in self.seen:
        return True
    self.seen.add(h)
    return False
```

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

# Data Sources

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

class FileSource:
â€œâ€â€œLoad code and text from local files.â€â€â€

```
EXTENSIONS = [".txt", ".md", ".py", ".js", ".php", ".java", ".json", ".ts", ".go"]

def __init__(self, input_dir: str):
    self.input_dir = Path(input_dir)
    self.files = []
    for ext in self.EXTENSIONS:
        self.files.extend(self.input_dir.rglob(f"*{ext}"))

    if not self.files:
        raise FileNotFoundError(f"No supported files found in {input_dir}")
    print(f"Found {len(self.files)} files in {input_dir}")

def iter_documents(self) -> Iterator[str]:
    for filepath in tqdm(self.files, desc="Reading files"):
        try:
            if filepath.suffix == ".json":
                with open(filepath, "r", encoding="utf-8", errors="ignore") as f:
                    data = json.load(f)
                if isinstance(data, list):
                    for item in data:
                        if isinstance(item, str):
                            yield item
                        elif isinstance(item, dict):
                            for key in ["content", "text", "code", "body"]:
                                if key in item:
                                    yield str(item[key])
                                    break
                elif isinstance(data, dict):
                    for key in ["content", "text", "code", "body"]:
                        if key in data:
                            yield str(data[key])
            else:
                with open(filepath, "r", encoding="utf-8", errors="ignore") as f:
                    yield f.read()
        except Exception as e:
            print(f"  Warning: Could not read {filepath}: {e}")
```

class WebSource:
â€œâ€â€œScrape text/code from a list of URLs.â€â€â€

```
def __init__(self, urls_file: str):
    try:
        import requests
        from bs4 import BeautifulSoup
        self.requests      = requests
        self.BeautifulSoup = BeautifulSoup
    except ImportError:
        raise ImportError("Run: pip install requests beautifulsoup4")

    with open(urls_file, "r") as f:
        self.urls = [l.strip() for l in f if l.strip() and not l.startswith("#")]
    print(f"Loaded {len(self.urls)} URLs")

def iter_documents(self) -> Iterator[str]:
    for url in tqdm(self.urls, desc="Scraping URLs"):
        try:
            headers = {"User-Agent": "Mozilla/5.0 (AfriCodeBot/1.0)"}
            resp    = self.requests.get(url, headers=headers, timeout=15)
            resp.raise_for_status()

            soup = self.BeautifulSoup(resp.text, "html.parser")
            for tag in soup(["script", "style", "nav", "footer", "header", "aside", "form", "ads"]):
                tag.decompose()

            # Extract code blocks specifically
            code_blocks = soup.find_all(["code", "pre"])
            code_text   = "\n".join(b.get_text() for b in code_blocks)

            # Extract main content
            main = soup.find("main") or soup.find("article") or soup.find("body")
            body_text = main.get_text(separator="\n") if main else ""

            # Combine code + body
            combined = code_text + "\n\n" + body_text
            if combined.strip():
                yield combined

        except Exception as e:
            print(f"  Warning: Failed to scrape {url}: {e}")
```

class AfriCodeSource:
â€œâ€â€œAuto-scrape the pre-defined list of African API documentation URLs.â€â€â€

```
def __init__(self):
    try:
        import requests
        from bs4 import BeautifulSoup
        self.requests      = requests
        self.BeautifulSoup = BeautifulSoup
    except ImportError:
        raise ImportError("Run: pip install requests beautifulsoup4")

    self.urls = AFRICODE_URLS
    print(f"AfriCode source: scraping {len(self.urls)} African API documentation URLs")

def iter_documents(self) -> Iterator[str]:
    for url in tqdm(self.urls, desc="Scraping African APIs"):
        try:
            headers = {"User-Agent": "Mozilla/5.0 (AfriCodeBot/1.0)"}
            resp    = self.requests.get(url, headers=headers, timeout=15)
            resp.raise_for_status()

            soup = self.BeautifulSoup(resp.text, "html.parser")
            for tag in soup(["script", "style", "nav", "footer", "header"]):
                tag.decompose()

            code_blocks = soup.find_all(["code", "pre"])
            code_text   = "\n".join(b.get_text() for b in code_blocks)

            main      = soup.find("main") or soup.find("article") or soup.find("body")
            body_text = main.get_text(separator="\n") if main else ""

            yield f"# Source: {url}\n\n{code_text}\n\n{body_text}"

        except Exception as e:
            print(f"  Warning: {url}: {e}")
```

class HuggingFaceSource:
â€œâ€â€œLoad a HuggingFace dataset. Defaults to code datasets.â€â€â€

```
def __init__(self, dataset_name: str, config: str = None):
    try:
        from datasets import load_dataset
        self.load_dataset = load_dataset
    except ImportError:
        raise ImportError("Run: pip install datasets")

    print(f"Loading HuggingFace dataset: {dataset_name} | config: {config}")
    self.dataset      = load_dataset(dataset_name, config, trust_remote_code=True)
    self.dataset_name = dataset_name

def iter_documents(self) -> Iterator[str]:
    split = "train" if "train" in self.dataset else list(self.dataset.keys())[0]
    data  = self.dataset[split]

    # Code datasets use "content", others use "text"
    text_keys = ["content", "text", "code", "body", "document"]

    for item in tqdm(data, desc=f"Loading {self.dataset_name}"):
        for key in text_keys:
            if key in item and item[key]:
                yield str(item[key])
                break
```

class QASource:
â€œâ€â€
Load Q&A pairs for fine-tuning.
Expects JSON files with format:
[
{â€œquestionâ€: â€œHow do I integrate M-Pesa?â€, â€œanswerâ€: â€œHere is the codeâ€¦â€},
â€¦
]
â€œâ€â€

```
def __init__(self, input_dir: str):
    self.input_dir = Path(input_dir)
    self.files     = list(self.input_dir.rglob("*.json"))
    if not self.files:
        raise FileNotFoundError(f"No JSON files found in {input_dir}")
    print(f"Found {len(self.files)} Q&A files")

def iter_documents(self) -> Iterator[str]:
    for filepath in tqdm(self.files, desc="Loading Q&A pairs"):
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                data = json.load(f)

            if isinstance(data, list):
                for item in data:
                    if "question" in item and "answer" in item:
                        # Format as instruction-following style
                        doc = (
                            f"### Question:\n{item['question']}\n\n"
                            f"### Answer:\n{item['answer']}\n"
                        )
                        yield doc
        except Exception as e:
            print(f"  Warning: {filepath}: {e}")
```

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

# Dataset Builder â€” Tokenize & Save as Binary

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

class DatasetBuilder:
â€œâ€â€œTokenize documents and save to binary files for fast training.â€â€â€

```
def __init__(self, output_dir: str, val_frac: float = 0.05):
    self.output_dir = Path(output_dir)
    self.output_dir.mkdir(parents=True, exist_ok=True)
    self.val_frac   = val_frac

    try:
        import tiktoken
        self.enc = tiktoken.get_encoding("gpt2")
    except ImportError:
        raise ImportError("Run: pip install tiktoken")

def build(self, documents: List[str], output_name: str = "africode"):
    print(f"\nTokenizing {len(documents):,} documents...")

    all_tokens  = []
    total_chars = 0

    for doc in tqdm(documents, desc="Tokenizing"):
        tokens = self.enc.encode(doc, allowed_special={"<|endoftext|>"})
        tokens.append(self.enc.eot_token)  # document separator
        all_tokens.extend(tokens)
        total_chars += len(doc)

    all_tokens  = np.array(all_tokens, dtype=np.uint16)
    total_tokens = len(all_tokens)
    split_idx   = int(total_tokens * (1 - self.val_frac))

    train_tokens = all_tokens[:split_idx]
    val_tokens   = all_tokens[split_idx:]

    train_path = self.output_dir / f"{output_name}_train.bin"
    val_path   = self.output_dir / f"{output_name}_val.bin"
    meta_path  = self.output_dir / f"{output_name}_meta.json"

    train_tokens.tofile(train_path)
    val_tokens.tofile(val_path)

    meta = {
        "dataset":         output_name,
        "total_tokens":    total_tokens,
        "train_tokens":    len(train_tokens),
        "val_tokens":      len(val_tokens),
        "total_chars":     total_chars,
        "total_documents": len(documents),
        "vocab_size":      self.enc.n_vocab,
        "encoding":        "gpt2",
    }
    with open(meta_path, "w") as f:
        json.dump(meta, f, indent=2)

    print(f"\n{'='*55}")
    print(f"  AfriCode Dataset Ready!")
    print(f"  Documents  : {len(documents):,}")
    print(f"  Characters : {total_chars:,}")
    print(f"  Tokens     : {total_tokens:,}")
    print(f"  Train      : {len(train_tokens):,} tokens â†’ {train_path}")
    print(f"  Val        : {len(val_tokens):,} tokens  â†’ {val_path}")
    print(f"{'='*55}\n")

    return str(train_path), str(val_path)
```

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

# Dataset Analysis

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

def analyze_dataset(documents: List[str]):
print(â€\nâ€” AfriCode Dataset Analysis â€”â€)
lengths     = [len(d) for d in documents]
total_chars = sum(lengths)
total_words = sum(len(d.split()) for d in documents)
token_est   = int(total_words * 1.3)

```
print(f"  Documents     : {len(documents):,}")
print(f"  Total chars   : {total_chars:,}")
print(f"  Avg doc length: {total_chars // max(len(documents), 1):,} chars")
print(f"  Est. tokens   : ~{token_est:,}")

print(f"\n--- Training Readiness ---")
if token_est < 1_000_000:
    print(f"  âš ï¸  Small ({token_est:,} tokens) â€” collect more data before serious training")
    print(f"  Tip: Add the-stack-smol Python dataset for instant volume boost")
elif token_est < 10_000_000:
    print(f"  âœ“  Good ({token_est:,} tokens) â€” solid start for a 10â€“25M param model")
elif token_est < 100_000_000:
    print(f"  âœ“âœ“ Large ({token_est:,} tokens) â€” ready for a serious 85M param model")
else:
    print(f"  ğŸš€ Excellent ({token_est:,} tokens) â€” you're playing in the big leagues!")
print()
```

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

# Main

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

def main():
args = parse_args()
os.makedirs(args.output_dir, exist_ok=True)

```
cleaner = TextCleaner(min_length=args.min_length, max_length=args.max_length)
deduper = Deduplicator()

# Select source
if args.source == "files":
    source = FileSource(args.input_dir)
elif args.source == "web":
    source = WebSource(args.urls)
elif args.source == "hf":
    source = HuggingFaceSource(args.dataset, args.dataset_config)
elif args.source == "qa":
    source = QASource(args.input_dir)
elif args.source == "africode":
    source = AfriCodeSource()

# Process documents
documents = []
stats     = {"total": 0, "low_quality": 0, "duplicate": 0, "accepted": 0}

print("\nProcessing documents...")
for raw_doc in source.iter_documents():
    stats["total"] += 1
    cleaned = cleaner.process(raw_doc)

    if cleaned is None:
        stats["low_quality"] += 1
        continue

    if deduper.is_duplicate(cleaned):
        stats["duplicate"] += 1
        continue

    documents.append(cleaned)
    stats["accepted"] += 1

print(f"\n  Total processed : {stats['total']:,}")
print(f"  Low quality     : {stats['low_quality']:,}")
print(f"  Duplicates      : {stats['duplicate']:,}")
print(f"  Accepted        : {stats['accepted']:,}")

if not documents:
    print("\nERROR: No documents passed the filter.")
    print("Try lowering --min_length or check your data source.")
    sys.exit(1)

if args.analyze:
    analyze_dataset(documents)

# Tokenize and save
builder = DatasetBuilder(output_dir=args.output_dir, val_frac=args.val_frac)
train_path, val_path = builder.build(documents, output_name=args.output)

print(f"âœ“ Pipeline complete! Train your model with:")
print(f"  python train.py --data_train {train_path} --data_val {val_path}")
```

if **name** == â€œ**main**â€:
main()
